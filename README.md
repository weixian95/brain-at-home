# Brain At Home
Expose a self-hosted Ollama chat endpoint to clients on the same private Tailscale network, with lightweight
memory, optional web search, and real-time UI state sync for multi-device chat.

## Components
- API gateway for chat, memory, titles/topics, and streaming responses.
- Web search service (Brave Search) that can enrich answers with sources.
- File-backed chat storage for history and metadata.
- Real-time sync layer that publishes chat list updates and shared UI state.

## Prompt Lifecycle (Per Request)
1. Receive `prompt`, `chat_id`, `model_id`, `use_web`, and metadata.
2. Classify if the prompt is info-seeking.
3. Decide web usage:
   - Web search runs only when `use_web=true` **and** the prompt is info-seeking or explicitly asks to search.
   - Otherwise, the server answers locally without web sources.
4. Build the model input:
   - **System prompt** (base instructions).
   - **Memory summary + facts** (short-term and long-term context).
   - **Recent turns** (latest user/assistant messages).
   - **Optional web sources** (only when web search is used).
   - **Web search query** is generated by a small model using the latest prompt (plus a few recent user turns if needed for context). If query generation fails or times out, the prompt text is used as the fallback query.
5. Answer:
   - If `use_web=false`: always answer with the local model (no web search sources).
   - If `use_web=true`: run web search only for info-seeking prompts; otherwise answer with the local model (no web search sources).
6. Stream responses (if `stream=true`) or return JSON.
7. Run post-answer tasks:
   - Update title/topic metadata.
   - Refresh memory summary and facts (when threshold is reached).
   - Optional polish pass for long responses.

## Chat Metadata and Timestamps
- **Titles** are generated once per chat using the title model. They update the history panel when available.
- **Topics** are refreshed after answers and streamed to clients for the topic bar.
- **History timestamps** are based on the latest user prompt time. They may update again after post-answer
  tasks (e.g., polish), which can shift the “last updated” display slightly later.

## Locking and Concurrency
- Requests are locked per chat (`withChatLock`), so multiple chats can run concurrently without
  corrupting shared history.
- The server tracks UI state per chat: `use_web`, `model_id`, `busy`, `input_disabled`, `history_locked`,
  `active`, and timestamps.
- Clients can choose how to interpret the global `busy` state. Chatty locks the history panel only for
  local sends and disables input when the current chat is busy.

## Tailscale Setup
1. Install Tailscale and log in.
2. On the host running Brain At Home, enable serve:
   - `npm run start:tailnet`
3. From another device on the same tailnet, call the API:
   - `https://<your-host>.tailnet.ts.net:3000/api/chat`

## Start Server
- `npm run start` (API)
- `npm run start:tailnet` (API + web search + Tailscale proxy)

Health check: `curl http://127.0.0.1:3000/health`

## Stop Server
```
npm run stop:tailnet
```

## API
All clients connected to the server share the same chat history.
- `GET /health`
- `POST /api/chat` (required: `chat_id`, `prompt`, `message_id`; optional: `model_id`, `stream`, `use_web`)
- `GET /api/tags` or `/api/models`
- `GET /api/chats`
- `GET /api/stream` (SSE: chat list + shared UI state)
- `GET /api/chats/:chat_id`
- `GET /api/chats/:chat_id/messages?offset=0&limit=50`
- `GET /api/chats/:chat_id/stream` (SSE: per-chat updates)
- `POST /api/chats/:chat_id/state` (sync UI state like `use_web`, `model_id`, `active`)
- `DELETE /api/chats/:chat_id`

### Realtime Sync (SSE)
`GET /api/stream` emits:
- `chatlist` snapshot `{ chats: [...] }`
- `chatlistupdate` `{ type: "added" | "updated" | "deleted", chat?, chat_id? }`
- `uistate` snapshot `{ global, chats }`
- `chatstate` updates `{ chat_id, state }`
- `globalstate` updates `{ active_chat_id, busy, busy_chats, input_disabled, history_locked }`

`GET /api/chats/:chat_id/stream` emits:
- `chatstate` (per-chat UI state, including `use_web`, `model_id`, `busy`, `input_disabled`)
- `globalstate` (global lock state)
- `chatinfoupdate` (title/topic/answer)

Example request:
```json
{
  "chat_id": "chat-001",
  "model_id": "llama3",
  "prompt": "What did I ask you last time?",
  "message_id": "9f7ad4c7-09e9-4e28-9e49-2c5c53f2d4e2",
  "client_ts": 1730775930123,
  "stream": false,
  "use_web": true
}
```

Response includes `sources` when the web search service is used:
```json
{
  "chat_id": "chat-001",
  "answer": "Answer with citations...",
  "sources": [
    { "title": "Example", "url": "https://example.com", "summary": "Short summary..." }
  ]
}
```
